{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.9\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.7.9 입장 방법 \n",
    "\n",
    "1. anaconda prompt 진입 \n",
    "2. activate py37 \n",
    "3. jupyter notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 일반적인 선형회귀 가정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 선형회귀를 가정한다\n",
    "- np.sum((beta[0] + beta[1]*x - y)**2) array 타입의 경우 최소제곱법 추정\n",
    "- np.mean((beta[0] + beta[1]*x - y)**2) MSE 처리시 \n",
    "- 리스트로 처리할 경우 루프처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,4]\n",
    "y = [0,-1,-3,-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 손실함수 정의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(beta,x,y):\n",
    "    n = len(x)\n",
    "    val = 0.0\n",
    "    \n",
    "    for i in range(n):\n",
    "        # np.sum((beta[0] + beta[1]*x - y)**2) array 타입의 경우 최소제곱법 추정\n",
    "        # np.mean((beta[0] + beta[1]*x - y)**2) MSE 처리시 \n",
    "        # 리스트로 처리할 경우 루프처리 \n",
    "        val += (beta[0] + beta[1]*x[i] - y[i])**2 / n\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 기울기 함수도 가정해보자 \n",
    "- val += (beta[0] + beta[1]* x[i] - y[i])**2 / n 를 미분해서\n",
    "- val += 2 * error * np.array([x[i], 1])/n 만들었다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_loss(beta,x,y):\n",
    "    dim = len(beta)\n",
    "    n = len(x)\n",
    "    val = np.array([0.0,0.0])\n",
    "    for i in range(n):\n",
    "        error = beta[0] + beta[1]*x[i] - y[i]\n",
    "        val += 2.0*error * np.array([1.0, x[i]]) / n\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 최적화 과정(Gradient Descent)\n",
    "- 기본적으로 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mse() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-de93f7f3ec5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mbeta1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeta0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mbeta0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: mse() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "maxIter = 1\n",
    "learning_rate = 0.01\n",
    "beta0 = np.array([0,0])\n",
    "\n",
    "for i in range(maxIter):\n",
    "    grad = grad_loss(beta0, x , y)\n",
    "    beta1 = beta0 - learning_rate * grad\n",
    "    beta0 = beta1 \n",
    "    print(beta0, mse(beta0, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 텐서 플로우를 이용한 선형회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 텐서플로우의 변수 \n",
    "- Constant (상수)\n",
    "- Variable (가변값)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 초기값 지정 \n",
    "- beta = tf.Variable([0,0], dtype=float64) 이렇게 지정할 수도 있음\n",
    "- 위에서 지정한 x와 y값을 그대로 이용하겠다\n",
    "- 은 아니고 constant 지정하겠다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,4]\n",
    "y = [0,-1,-3,-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([1,2,3,4], dtype=tf.float64)\n",
    "y = tf.constant([0,-1,-3,-3], dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable([0], dtype=tf.float64) # 기울기 x에 대한, beta1, w1, 변수의 영향력(회귀계수)\n",
    "b = tf.Variable([0], dtype=tf.float64) # y절편, beta0, bias(편향)\n",
    "\n",
    "beta = tf.Variable([0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 손실함수와 최적화 함수 정의\n",
    "- 리니어모델 , 손실함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(x):\n",
    "    return b + a*x \n",
    "\n",
    "\n",
    "def mse(yhat, y):\n",
    "    return tf.reduce_sum(tf.pow(yhat-y,2)) / ( x.shape[0]) #*2 \n",
    "    # reduce_mean 하고 분수 없애기 \n",
    "    \n",
    "optimizer = tf.optimizers.SGD(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 최적화 과정 정의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization():\n",
    "    #  주어진 입력변수에 대해서 자동으로 기울기를 구해준다. \n",
    "    with tf.GradientTape() as grad: \n",
    "        yhat = linear_model(x)\n",
    "        loss = mse(yhat,y)\n",
    "        \n",
    "        \n",
    "    # 기울기 계산 \n",
    "    gradients = grad.gradient( loss, [ a , b ] )\n",
    "    \n",
    "    # a,b 업데이트 \n",
    "    optimizer.apply_gradients( zip( gradients, [ a, b ] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- beta0 = b \n",
    "- beta1 = 기울기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 학습(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0, loss: 3.425537526909821, a: <tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([-0.115])>, b; <tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([-0.035])>\n"
     ]
    }
   ],
   "source": [
    "maxIter = 1\n",
    "for i in range(maxIter):\n",
    "    optimization()\n",
    "    yhat = linear_model(x)\n",
    "    loss = mse(yhat, y)\n",
    "    print('step : {}, loss: {}, a: {}, b; {}'.format(i, loss, a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    optimization()\n",
    "    yhat = linear_model(x)\n",
    "    loss = mse(yhat, y)\n",
    "    \n",
    "    제외하고 최적화 과정을 투입해도 같은 결과가 도출됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0, loss: 3.425537526909821, a: <tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([-0.211])>, b; <tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([-0.06355])>\n"
     ]
    }
   ],
   "source": [
    " maxIter = 10\n",
    "for i in range(maxIter):\n",
    "    with tf.GradientTape() as grad: \n",
    "        yhat = linear_model(x)\n",
    "        loss = mse(yhat,y)\n",
    "        \n",
    "    # 기울기 계산 \n",
    "    gradients = grad.gradient( loss, [ a , b ] )\n",
    "    \n",
    "    # a,b 업데이트 \n",
    "    optimizer.apply_gradients( zip( gradients, [ a, b ] ) )\n",
    "    print('step : {}, loss: {}, a: {}, b; {}'.format(i, loss, a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 원래 했던 대로 np.array로 계산한다면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "y = np.array([0,-1,-3,-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta0 = np.array([0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 손실함수와 최적화 함수 정의\n",
    "- 리니어모델 , 손실함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(beta):\n",
    "    yhat = beta[0] + beta[1]*x \n",
    "    \n",
    "    return np.mean((yhat-y)**2)\n",
    "\n",
    "# 텐서에서는 MSE와 리니어 모델로 분리 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 최적화 과정 정의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization():\n",
    "    #  주어진 입력변수에 대해서 자동으로 기울기를 구해준다. \n",
    "    with tf.GradientTape() as grad: \n",
    "        yhat = linear_model(x)\n",
    "        loss = mse(yhat,y)\n",
    "        \n",
    "        \n",
    "    # 기울기 계산 \n",
    "    gradients = grad.gradient( loss, [ a , b ] )\n",
    "    \n",
    "    # a,b 업데이트 \n",
    "    optimizer.apply_gradients( zip( gradients, [ a, b ] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- beta0 = b \n",
    "- beta1 = 기울기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 학습(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    optimization()\n",
    "    yhat = linear_model(x)\n",
    "    loss = mse(yhat, y)\n",
    "    \n",
    "    제외하고 최적화 과정을 투입해도 같은 결과가 도출됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0, loss: 3.425537526909821, a: <tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([-0.211])>, b; <tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([-0.06355])>\n",
      "step : 1, loss: 3.425537526909821, a: <tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([-0.211])>, b; <tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([-0.06355])>\n",
      "step : 2, loss: 3.425537526909821, a: <tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([-0.211])>, b; <tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([-0.06355])>\n",
      "step : 3, loss: 3.425537526909821, a: <tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([-0.211])>, b; <tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([-0.06355])>\n",
      "step : 4, loss: 3.425537526909821, a: <tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([-0.211])>, b; <tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([-0.06355])>\n",
      "step : 5, loss: 3.425537526909821, a: <tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([-0.211])>, b; <tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([-0.06355])>\n",
      "step : 6, loss: 3.425537526909821, a: <tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([-0.211])>, b; <tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([-0.06355])>\n",
      "step : 7, loss: 3.425537526909821, a: <tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([-0.211])>, b; <tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([-0.06355])>\n",
      "step : 8, loss: 3.425537526909821, a: <tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([-0.211])>, b; <tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([-0.06355])>\n",
      "step : 9, loss: 3.425537526909821, a: <tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([-0.211])>, b; <tf.Variable 'Variable:0' shape=(1,) dtype=float64, numpy=array([-0.06355])>\n"
     ]
    }
   ],
   "source": [
    "maxIter = 10\n",
    "for i in range(maxIter):\n",
    "    # 기울기 함수를 통해 기울기 계산(loss , a, b)\n",
    "    grad = grad_loss(beta0,x,y)\n",
    "    beta1 = beta0 - learning_rate * grad \n",
    "\n",
    "    # a,b 업데이트 \n",
    "    beta0 = beta1 \n",
    "    \n",
    "    # 출력 \n",
    "    print('step : {}, loss: {}, a: {}, b; {}'.format(i, loss, a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
